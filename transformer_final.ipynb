{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37686/4114127383.py:1: DeprecationWarning: 'cgi' is deprecated and slated for removal in Python 3.13\n",
      "  from cgi import test\n",
      "/home/dnull/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from cgi import test\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_path = \"./data/deepfashion-multimodal/images/\"\n",
    "train_text = \"./data/deepfashion-multimodal/train_captions.json\"\n",
    "test_text = \"./data/deepfashion-multimodal/test_captions.json\"\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_paths, train_captions, test_captions, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.train_captions = train_captions\n",
    "        self.test_captions = test_captions\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_name = self.image_paths[idx]\n",
    "        file_name = idx_name.split(\"/\")[-1]\n",
    "        image = Image.open(idx_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.train_captions[file_name]\n",
    "    \n",
    "    def get_train_captions(self):\n",
    "        return self.train_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train_captions = json.load(open(train_text, 'r'))\n",
    "test_captions = json.load(open(test_text, 'r'))\n",
    "image_paths = []\n",
    "# add from train_captions\n",
    "for key in train_captions.keys():\n",
    "    image_paths.append(image_path + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WOMEN-Jackets_Coats-id_00005611-01_4_full.jpg [159, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 13, 14, 15, 16, 17, 1, 18, 3, 16, 14, 5, 19, 1, 8, 16, 20, 9, 15, 4, 10, 11, 12, 21, 22, 23, 24, 25, 26, 27, 28, 7, 8, 9, 29, 12, 21, 30, 16, 31, 32, 33, 34, 35, 36, 21, 30, 4, 37, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "WOMEN-Tees_Tanks-id_00005033-03_4_full.jpg [159, 38, 39, 40, 4, 41, 6, 42, 8, 9, 43, 12, 44, 4, 32, 45, 46, 1, 47, 24, 32, 5, 48, 1, 49, 50, 28, 20, 8, 9, 10, 11, 12, 1, 22, 24, 32, 51, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "WOMEN-Rompers_Jumpsuits-id_00000245-01_1_front.jpg [159, 38, 39, 52, 4, 41, 6, 7, 8, 9, 10, 11, 12, 44, 4, 32, 53, 46, 21, 54, 24, 32, 5, 55, 1, 56, 50, 28, 7, 8, 9, 10, 11, 12, 57, 16, 32, 33, 34, 35, 36, 1, 22, 24, 32, 58, 57, 16, 25, 59, 34, 35, 60, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 2\n",
      "WOMEN-Blouses_Shirts-id_00002333-02_7_additional.jpg [159, 38, 61, 4, 5, 6, 7, 8, 9, 10, 11, 12, 44, 4, 32, 45, 46, 21, 54, 24, 32, 5, 48, 1, 49, 50, 28, 7, 8, 9, 62, 12, 57, 16, 25, 59, 34, 35, 60, 57, 16, 32, 33, 34, 35, 36, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 3\n",
      "WOMEN-Blouses_Shirts-id_00002102-04_4_full.jpg [159, 1, 22, 16, 31, 32, 39, 39, 40, 28, 43, 12, 1, 39, 40, 16, 28, 7, 63, 1, 49, 64, 22, 24, 16, 14, 65, 19, 1, 49, 50, 28, 20, 8, 9, 66, 11, 12, 57, 16, 25, 59, 34, 35, 60, 57, 16, 32, 33, 34, 35, 36, 57, 16, 25, 59, 67, 68, 35, 69, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 4\n"
     ]
    }
   ],
   "source": [
    "image_dic = train_captions.keys()\n",
    "image_descriptions = train_captions.values()\n",
    "\n",
    "# build vocabulary\n",
    "vocab = Counter()\n",
    "for description in image_descriptions:\n",
    "    vocab.update(description.split())\n",
    "\n",
    "# remove words that occur less than threshold\n",
    "threshold = -1\n",
    "words = [word for word, count in vocab.items() if count >= threshold]\n",
    "\n",
    "# create a mapping from word to index and index to word\n",
    "idx_to_word = {idx: word for idx, word in enumerate(words, 1)}\n",
    "\n",
    "# add the start and end token to the vocabulary\n",
    "idx_to_word[0] = \"<pad>\"\n",
    "idx_to_word[len(idx_to_word)] = \"<end>\"\n",
    "idx_to_word[len(idx_to_word)] = \"<start>\"\n",
    "\n",
    "# add the end to the end pos\n",
    "for key, description in train_captions.items():\n",
    "    train_captions[key] = \"<start> \" + description + \" <end>\"\n",
    "\n",
    "# pad the descriptions with <pad>\n",
    "max_length = max(len(description.split()) for description in image_descriptions)\n",
    "for key, description in train_captions.items():\n",
    "    train_captions[key] = description + \" <pad>\" * (\n",
    "        max_length - len(description.split())\n",
    "    )\n",
    "for key, description in test_captions.items():\n",
    "    test_captions[key] = description + \" <pad>\" * (\n",
    "        max_length - len(description.split())\n",
    "    )\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in idx_to_word.items()}\n",
    "vocab_size_len = len(idx_to_word)\n",
    "\n",
    "\n",
    "# convert each word to its index\n",
    "train_captions = {\n",
    "    key: [word_to_idx[word] for word in value.split()]\n",
    "    for key, value in train_captions.items()\n",
    "}\n",
    "test_captions = {\n",
    "    key: [word_to_idx[word] for word in value.split()]\n",
    "    for key, value in test_captions.items()\n",
    "}\n",
    "\n",
    "\n",
    "for key, value, idx in zip(train_captions.keys(), train_captions.values(), range(5)):\n",
    "    print(key, value, idx)\n",
    "    if idx == 4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256]) torch.Size([95])\n",
      "tensor([[[2.2318, 2.2318, 2.2318,  ..., 2.1804, 2.1804, 2.1804],\n",
      "         [2.2318, 2.2147, 2.2318,  ..., 2.1804, 2.1804, 2.1804],\n",
      "         [2.2318, 2.2318, 2.2318,  ..., 2.1804, 2.1804, 2.1804],\n",
      "         ...,\n",
      "         [2.1804, 2.1633, 2.1633,  ..., 2.0777, 2.0777, 2.0777],\n",
      "         [2.1804, 2.1633, 2.1633,  ..., 2.0948, 2.0777, 2.0777],\n",
      "         [2.1804, 2.1804, 2.1633,  ..., 2.0948, 2.0948, 2.0777]],\n",
      "\n",
      "        [[2.4111, 2.4111, 2.4111,  ..., 2.3585, 2.3585, 2.3585],\n",
      "         [2.4111, 2.3936, 2.4111,  ..., 2.3585, 2.3585, 2.3585],\n",
      "         [2.4111, 2.4111, 2.4111,  ..., 2.3585, 2.3585, 2.3585],\n",
      "         ...,\n",
      "         [2.3235, 2.3410, 2.3410,  ..., 2.2185, 2.2185, 2.2185],\n",
      "         [2.3235, 2.3410, 2.3410,  ..., 2.2360, 2.2185, 2.2185],\n",
      "         [2.3235, 2.3410, 2.3410,  ..., 2.2360, 2.2360, 2.2185]],\n",
      "\n",
      "        [[2.6226, 2.6226, 2.6226,  ..., 2.5703, 2.5703, 2.5703],\n",
      "         [2.6226, 2.6051, 2.6226,  ..., 2.5703, 2.5703, 2.5703],\n",
      "         [2.6226, 2.6226, 2.6226,  ..., 2.5703, 2.5703, 2.5703],\n",
      "         ...,\n",
      "         [2.5529, 2.5529, 2.5529,  ..., 2.4483, 2.4483, 2.4483],\n",
      "         [2.5529, 2.5529, 2.5529,  ..., 2.4657, 2.4483, 2.4483],\n",
      "         [2.5529, 2.5529, 2.5529,  ..., 2.4483, 2.4657, 2.4483]]]) tensor([159,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,   1,\n",
      "         13,  14,  15,  16,  17,   1,  18,   3,  16,  14,   5,  19,   1,   8,\n",
      "         16,  20,   9,  15,   4,  10,  11,  12,  21,  22,  23,  24,  25,  26,\n",
      "         27,  28,   7,   8,   9,  29,  12,  21,  30,  16,  31,  32,  33,  34,\n",
      "         35,  36,  21,  30,   4,  37, 158,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key, value in train_captions.items():\n",
    "    train_captions[key] = torch.tensor(value, dtype=torch.long)\n",
    "\n",
    "# make dataset\n",
    "dataset = MyDataset(image_paths, train_captions, test_captions, transform)\n",
    "\n",
    "for i in range(1):\n",
    "    print(dataset[i][0].shape, dataset[i][1].shape)\n",
    "    print(dataset[i][0], dataset[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn import Transformer\n",
    "\n",
    "\n",
    "# 1. 使用预训练的 CNN 提取特征\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        \n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "\n",
    "    def forward(self, images):\n",
    "        # resize to 3 times larger\n",
    "        batch_size = images.shape[0]\n",
    "        images = nn.functional.interpolate(\n",
    "            images, scale_factor=3, mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        images = images.unfold(2, 256, 256).unfold(3, 256, 256)\n",
    "        images = images.contiguous().view(-1, 3, 256, 256)\n",
    "\n",
    "        # 提取特征\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(batch_size, 9, -1)\n",
    "        return features\n",
    "\n",
    "\n",
    "# 2. 构建 Transformer 模型\n",
    "class ImageCaptioningTransformer(nn.Module):\n",
    "    def __init__(self, emb_dim, nhead, nhid, nlayers, vocab_size, max_seq_length):\n",
    "        super(ImageCaptioningTransformer, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim)\n",
    "        self.transformer = Transformer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=nhid,\n",
    "            num_encoder_layers=nlayers,\n",
    "            num_decoder_layers=nlayers,\n",
    "            batch_first=False,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.decoder = nn.Linear(emb_dim, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None,\n",
    "                tgt_padding_mask=None, memory_mask=None):\n",
    "        src = self.pos_encoder(src)  # 添加位置编码\n",
    "        tgt = self.embedding(tgt)  # 词嵌入\n",
    "        tgt = self.pos_encoder(tgt)  # 添加位置编码\n",
    "\n",
    "        output = self.transformer(src, tgt, src_mask, tgt_mask, None, src_padding_mask,\n",
    "                tgt_padding_mask, memory_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 辅助类：位置编码\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 创建位置编码\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device='cuda')) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device='cuda').type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == 0).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == 0).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnull/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dnull/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/dnull/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnull/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 5.289523601531982\n",
      "epoch: 0, step: 10, loss: 4.152801036834717\n",
      "epoch: 0, step: 20, loss: 4.036811828613281\n",
      "epoch: 0, step: 30, loss: 2.8322768211364746\n",
      "epoch: 0, step: 40, loss: 1.4876823425292969\n",
      "epoch: 0, step: 50, loss: 1.0513161420822144\n",
      "epoch: 0, step: 60, loss: 0.9210466742515564\n",
      "epoch: 0, step: 70, loss: 0.906835675239563\n",
      "epoch: 0, step: 80, loss: 0.7109276652336121\n",
      "epoch: 0, step: 90, loss: 0.8254294991493225\n",
      "epoch: 0, step: 100, loss: 0.685184895992279\n",
      "epoch: 0, step: 110, loss: 0.68462073802948\n",
      "epoch: 0, step: 120, loss: 0.6383317112922668\n",
      "epoch: 0, step: 130, loss: 0.6734803915023804\n",
      "epoch: 0, step: 140, loss: 0.6619338393211365\n",
      "epoch: 0, step: 150, loss: 0.6647012829780579\n",
      "epoch: 0, step: 160, loss: 0.6635385155677795\n",
      "epoch: 0, step: 170, loss: 0.6401270627975464\n",
      "epoch: 0, step: 180, loss: 0.6715878844261169\n",
      "epoch: 0, step: 190, loss: 0.6044638752937317\n",
      "epoch: 0, step: 200, loss: 0.6118202209472656\n",
      "epoch: 0, step: 210, loss: 0.6002620458602905\n",
      "epoch: 0, step: 220, loss: 0.5670754313468933\n",
      "epoch: 0, step: 230, loss: 0.5376838445663452\n",
      "epoch: 0, step: 240, loss: 0.6026570796966553\n",
      "epoch: 0, step: 250, loss: 0.679874837398529\n",
      "epoch: 0, step: 260, loss: 0.5731520652770996\n",
      "epoch: 0, step: 270, loss: 0.5790445804595947\n",
      "epoch: 0, step: 280, loss: 0.5891852378845215\n",
      "epoch: 0, step: 290, loss: 0.5327874422073364\n",
      "epoch: 0, step: 300, loss: 0.5493288040161133\n",
      "epoch: 0, step: 310, loss: 0.5824686884880066\n",
      "epoch: 0, step: 320, loss: 0.5659689903259277\n",
      "epoch: 0, step: 330, loss: 0.5561832785606384\n",
      "epoch: 0, step: 340, loss: 0.5677450895309448\n",
      "epoch: 0, step: 350, loss: 0.4989739954471588\n",
      "epoch: 0, step: 360, loss: 0.5046417117118835\n",
      "epoch: 0, step: 370, loss: 0.5898839235305786\n",
      "epoch: 0, step: 380, loss: 0.5953059196472168\n",
      "epoch: 0, step: 390, loss: 0.5676910877227783\n",
      "epoch: 0, step: 400, loss: 0.5452579259872437\n",
      "epoch: 0, step: 410, loss: 0.5605825781822205\n",
      "epoch: 0, step: 420, loss: 0.5802826881408691\n",
      "epoch: 0, step: 430, loss: 0.5676610469818115\n",
      "epoch: 0, step: 440, loss: 0.538558840751648\n",
      "epoch: 0, step: 450, loss: 0.5858771204948425\n",
      "epoch: 0, step: 460, loss: 0.5259072184562683\n",
      "epoch: 0, step: 470, loss: 0.5688080787658691\n",
      "epoch: 0, step: 480, loss: 0.5682494044303894\n",
      "epoch: 0, step: 490, loss: 0.4685022234916687\n",
      "epoch: 0, step: 500, loss: 0.5611436367034912\n",
      "epoch: 0, step: 510, loss: 0.5877838730812073\n",
      "epoch: 0, step: 520, loss: 0.5744625926017761\n",
      "epoch: 0, step: 530, loss: 0.5035926699638367\n",
      "epoch: 0, step: 540, loss: 0.5467451214790344\n",
      "epoch: 0, step: 550, loss: 0.5001705288887024\n",
      "epoch: 0, step: 560, loss: 0.566103458404541\n",
      "epoch: 0, step: 570, loss: 0.5141871571540833\n",
      "epoch: 0, step: 580, loss: 0.5583635568618774\n",
      "epoch: 0, step: 590, loss: 0.5807813405990601\n",
      "epoch: 0, step: 600, loss: 0.5636563897132874\n",
      "epoch: 0, step: 610, loss: 0.5890935063362122\n",
      "epoch: 0, step: 620, loss: 0.5533685088157654\n",
      "epoch: 0, step: 630, loss: 0.5515359044075012\n",
      "epoch: 1, step: 0, loss: 0.6487793326377869\n",
      "epoch: 1, step: 10, loss: 0.5784484148025513\n",
      "epoch: 1, step: 20, loss: 0.532355546951294\n",
      "epoch: 1, step: 30, loss: 0.5461688041687012\n",
      "epoch: 1, step: 40, loss: 0.5854038596153259\n",
      "epoch: 1, step: 50, loss: 0.5370216965675354\n",
      "epoch: 1, step: 60, loss: 0.520280659198761\n",
      "epoch: 1, step: 70, loss: 0.584190309047699\n",
      "epoch: 1, step: 80, loss: 0.5276176929473877\n",
      "epoch: 1, step: 90, loss: 0.6034827828407288\n",
      "epoch: 1, step: 100, loss: 0.5317039489746094\n",
      "epoch: 1, step: 110, loss: 0.5571556687355042\n",
      "epoch: 1, step: 120, loss: 0.5830836892127991\n",
      "epoch: 1, step: 130, loss: 0.5463500022888184\n",
      "epoch: 1, step: 140, loss: 0.48984113335609436\n",
      "epoch: 1, step: 150, loss: 0.5378915071487427\n",
      "epoch: 1, step: 160, loss: 0.5230805277824402\n",
      "epoch: 1, step: 170, loss: 0.5830944776535034\n",
      "epoch: 1, step: 180, loss: 0.49850261211395264\n",
      "epoch: 1, step: 190, loss: 0.5487588047981262\n",
      "epoch: 1, step: 200, loss: 0.5532497763633728\n",
      "epoch: 1, step: 210, loss: 0.5635072588920593\n",
      "epoch: 1, step: 220, loss: 0.5278102159500122\n",
      "epoch: 1, step: 230, loss: 0.5624147653579712\n",
      "epoch: 1, step: 240, loss: 0.5432444214820862\n",
      "epoch: 1, step: 250, loss: 0.4824983477592468\n",
      "epoch: 1, step: 260, loss: 0.5432024002075195\n",
      "epoch: 1, step: 270, loss: 0.5222895741462708\n",
      "epoch: 1, step: 280, loss: 0.5207291841506958\n",
      "epoch: 1, step: 290, loss: 0.561581552028656\n",
      "epoch: 1, step: 300, loss: 0.4976772665977478\n",
      "epoch: 1, step: 310, loss: 0.6098461151123047\n",
      "epoch: 1, step: 320, loss: 0.5599428415298462\n",
      "epoch: 1, step: 330, loss: 0.5945478677749634\n",
      "epoch: 1, step: 340, loss: 0.5456578135490417\n",
      "epoch: 1, step: 350, loss: 0.5140252113342285\n",
      "epoch: 1, step: 360, loss: 0.5692726969718933\n",
      "epoch: 1, step: 370, loss: 0.5241648554801941\n",
      "epoch: 1, step: 380, loss: 0.5505406260490417\n",
      "epoch: 1, step: 390, loss: 0.6042296290397644\n",
      "epoch: 1, step: 400, loss: 0.5539588332176208\n",
      "epoch: 1, step: 410, loss: 0.5246796607971191\n",
      "epoch: 1, step: 420, loss: 0.5339731574058533\n",
      "epoch: 1, step: 430, loss: 0.5663185119628906\n",
      "epoch: 1, step: 440, loss: 0.558062732219696\n",
      "epoch: 1, step: 450, loss: 0.663378119468689\n",
      "epoch: 1, step: 460, loss: 0.536316454410553\n",
      "epoch: 1, step: 470, loss: 0.5501281023025513\n",
      "epoch: 1, step: 480, loss: 0.5628787279129028\n",
      "epoch: 1, step: 490, loss: 0.5139325261116028\n",
      "epoch: 1, step: 500, loss: 0.5402493476867676\n",
      "epoch: 1, step: 510, loss: 0.569579541683197\n",
      "epoch: 1, step: 520, loss: 0.5273305773735046\n",
      "epoch: 1, step: 530, loss: 0.5217407941818237\n",
      "epoch: 1, step: 540, loss: 0.5294736623764038\n",
      "epoch: 1, step: 550, loss: 0.5745720863342285\n",
      "epoch: 1, step: 560, loss: 0.5914777517318726\n",
      "epoch: 1, step: 570, loss: 0.5362248420715332\n",
      "epoch: 1, step: 580, loss: 0.5791037678718567\n",
      "epoch: 1, step: 590, loss: 0.576023280620575\n",
      "epoch: 1, step: 600, loss: 0.5221606492996216\n",
      "epoch: 1, step: 610, loss: 0.5228169560432434\n",
      "epoch: 1, step: 620, loss: 0.5604565739631653\n",
      "epoch: 1, step: 630, loss: 0.5622102618217468\n",
      "epoch: 2, step: 0, loss: 0.5440729856491089\n",
      "epoch: 2, step: 10, loss: 0.5522645115852356\n",
      "epoch: 2, step: 20, loss: 0.5358786582946777\n",
      "epoch: 2, step: 30, loss: 0.5162274241447449\n",
      "epoch: 2, step: 40, loss: 0.5731116533279419\n",
      "epoch: 2, step: 50, loss: 0.5978184938430786\n",
      "epoch: 2, step: 60, loss: 0.6029530167579651\n",
      "epoch: 2, step: 70, loss: 0.5212640762329102\n",
      "epoch: 2, step: 80, loss: 0.6210094094276428\n",
      "epoch: 2, step: 90, loss: 0.6161150932312012\n",
      "epoch: 2, step: 100, loss: 0.5954388976097107\n",
      "epoch: 2, step: 110, loss: 0.6018192172050476\n",
      "epoch: 2, step: 120, loss: 0.6380045413970947\n",
      "epoch: 2, step: 130, loss: 0.5402466058731079\n",
      "epoch: 2, step: 140, loss: 0.5610390305519104\n",
      "epoch: 2, step: 150, loss: 0.5413292646408081\n",
      "epoch: 2, step: 160, loss: 0.5935887694358826\n",
      "epoch: 2, step: 170, loss: 0.5538461208343506\n",
      "epoch: 2, step: 180, loss: 0.5981069803237915\n",
      "epoch: 2, step: 190, loss: 0.5695877075195312\n",
      "epoch: 2, step: 200, loss: 0.5367717146873474\n",
      "epoch: 2, step: 210, loss: 0.6284692287445068\n",
      "epoch: 2, step: 220, loss: 0.5741727948188782\n",
      "epoch: 2, step: 230, loss: 0.5341513156890869\n",
      "epoch: 2, step: 240, loss: 0.5345741510391235\n",
      "epoch: 2, step: 250, loss: 0.510211169719696\n",
      "epoch: 2, step: 260, loss: 0.5597795248031616\n",
      "epoch: 2, step: 270, loss: 0.5188655257225037\n",
      "epoch: 2, step: 280, loss: 0.5813599228858948\n",
      "epoch: 2, step: 290, loss: 0.5456595420837402\n",
      "epoch: 2, step: 300, loss: 0.6467727422714233\n",
      "epoch: 2, step: 310, loss: 0.5783133506774902\n",
      "epoch: 2, step: 320, loss: 0.5808271169662476\n",
      "epoch: 2, step: 330, loss: 0.5534775257110596\n",
      "epoch: 2, step: 340, loss: 0.5919288992881775\n",
      "epoch: 2, step: 350, loss: 0.5961649417877197\n",
      "epoch: 2, step: 360, loss: 0.5815627574920654\n",
      "epoch: 2, step: 370, loss: 0.6013960242271423\n",
      "epoch: 2, step: 380, loss: 0.6147977113723755\n",
      "epoch: 2, step: 390, loss: 0.5277310609817505\n",
      "epoch: 2, step: 400, loss: 0.5895475745201111\n",
      "epoch: 2, step: 410, loss: 0.5994935631752014\n",
      "epoch: 2, step: 420, loss: 0.5679261088371277\n",
      "epoch: 2, step: 430, loss: 0.6355751156806946\n",
      "epoch: 2, step: 440, loss: 0.6090846061706543\n",
      "epoch: 2, step: 450, loss: 0.6556769609451294\n",
      "epoch: 2, step: 460, loss: 0.595119297504425\n",
      "epoch: 2, step: 470, loss: 0.6011651754379272\n",
      "epoch: 2, step: 480, loss: 0.6635079383850098\n",
      "epoch: 2, step: 490, loss: 0.6400622725486755\n",
      "epoch: 2, step: 500, loss: 0.6855747699737549\n",
      "epoch: 2, step: 510, loss: 0.6029972434043884\n",
      "epoch: 2, step: 520, loss: 0.6336303949356079\n",
      "epoch: 2, step: 530, loss: 0.6070085167884827\n",
      "epoch: 2, step: 540, loss: 0.5746902227401733\n",
      "epoch: 2, step: 550, loss: 0.6137818098068237\n",
      "epoch: 2, step: 560, loss: 0.6327265501022339\n",
      "epoch: 2, step: 570, loss: 0.5984846353530884\n",
      "epoch: 2, step: 580, loss: 0.5816782116889954\n",
      "epoch: 2, step: 590, loss: 0.6092267632484436\n",
      "epoch: 2, step: 600, loss: 0.5839695334434509\n",
      "epoch: 2, step: 610, loss: 0.5321789979934692\n",
      "epoch: 2, step: 620, loss: 0.554240882396698\n",
      "epoch: 2, step: 630, loss: 0.6120379567146301\n",
      "epoch: 3, step: 0, loss: 0.6199204325675964\n",
      "epoch: 3, step: 10, loss: 0.5950151681900024\n",
      "epoch: 3, step: 20, loss: 0.6021698117256165\n",
      "epoch: 3, step: 30, loss: 0.6233106255531311\n",
      "epoch: 3, step: 40, loss: 0.5890628695487976\n",
      "epoch: 3, step: 50, loss: 0.6096792221069336\n",
      "epoch: 3, step: 60, loss: 0.6582103967666626\n",
      "epoch: 3, step: 70, loss: 0.6287303566932678\n",
      "epoch: 3, step: 80, loss: 0.5993162393569946\n",
      "epoch: 3, step: 90, loss: 0.5526612401008606\n",
      "epoch: 3, step: 100, loss: 0.6081191897392273\n",
      "epoch: 3, step: 110, loss: 0.5821821689605713\n",
      "epoch: 3, step: 120, loss: 0.5461747646331787\n",
      "epoch: 3, step: 130, loss: 0.6061506271362305\n",
      "epoch: 3, step: 140, loss: 0.5331369638442993\n",
      "epoch: 3, step: 150, loss: 0.6177117228507996\n",
      "epoch: 3, step: 160, loss: 0.690613865852356\n",
      "epoch: 3, step: 170, loss: 0.5966529250144958\n",
      "epoch: 3, step: 180, loss: 0.6874280571937561\n",
      "epoch: 3, step: 190, loss: 0.5981169939041138\n",
      "epoch: 3, step: 200, loss: 0.5900536179542542\n",
      "epoch: 3, step: 210, loss: 0.5601387023925781\n",
      "epoch: 3, step: 220, loss: 0.59139484167099\n",
      "epoch: 3, step: 230, loss: 0.6308732032775879\n",
      "epoch: 3, step: 240, loss: 0.5755347609519958\n",
      "epoch: 3, step: 250, loss: 0.6552755236625671\n",
      "epoch: 3, step: 260, loss: 0.5812955498695374\n",
      "epoch: 3, step: 270, loss: 0.6297051906585693\n",
      "epoch: 3, step: 280, loss: 0.553396463394165\n",
      "epoch: 3, step: 290, loss: 0.7270693182945251\n",
      "epoch: 3, step: 300, loss: 0.5889042019844055\n",
      "epoch: 3, step: 310, loss: 0.6177459955215454\n",
      "epoch: 3, step: 320, loss: 0.6111487746238708\n",
      "epoch: 3, step: 330, loss: 0.6662744879722595\n",
      "epoch: 3, step: 340, loss: 0.5733861327171326\n",
      "epoch: 3, step: 350, loss: 0.6084112524986267\n",
      "epoch: 3, step: 360, loss: 0.5877197980880737\n",
      "epoch: 3, step: 370, loss: 0.6349411606788635\n",
      "epoch: 3, step: 380, loss: 0.6186219453811646\n",
      "epoch: 3, step: 390, loss: 0.6171925067901611\n",
      "epoch: 3, step: 400, loss: 0.5907005667686462\n",
      "epoch: 3, step: 410, loss: 0.6094471216201782\n",
      "epoch: 3, step: 420, loss: 0.5880627632141113\n",
      "epoch: 3, step: 430, loss: 0.5258076190948486\n",
      "epoch: 3, step: 440, loss: 0.594099223613739\n",
      "epoch: 3, step: 450, loss: 0.6680153012275696\n",
      "epoch: 3, step: 460, loss: 0.6084341406822205\n",
      "epoch: 3, step: 470, loss: 0.6144263744354248\n",
      "epoch: 3, step: 480, loss: 0.6280181407928467\n",
      "epoch: 3, step: 490, loss: 0.5628187656402588\n",
      "epoch: 3, step: 500, loss: 0.6491053700447083\n",
      "epoch: 3, step: 510, loss: 0.626974880695343\n",
      "epoch: 3, step: 520, loss: 0.6888258457183838\n",
      "epoch: 3, step: 530, loss: 0.5739888548851013\n",
      "epoch: 3, step: 540, loss: 0.5832712650299072\n",
      "epoch: 3, step: 550, loss: 0.5542421936988831\n",
      "epoch: 3, step: 560, loss: 0.6257691979408264\n",
      "epoch: 3, step: 570, loss: 0.5809865593910217\n",
      "epoch: 3, step: 580, loss: 0.6010714769363403\n",
      "epoch: 3, step: 590, loss: 0.5563373565673828\n",
      "epoch: 3, step: 600, loss: 0.5299266576766968\n",
      "epoch: 3, step: 610, loss: 0.566849946975708\n",
      "epoch: 3, step: 620, loss: 0.552052915096283\n",
      "epoch: 3, step: 630, loss: 0.5858256220817566\n",
      "epoch: 4, step: 0, loss: 0.4904980957508087\n",
      "epoch: 4, step: 10, loss: 0.5371827483177185\n",
      "epoch: 4, step: 20, loss: 0.6182154417037964\n",
      "epoch: 4, step: 30, loss: 0.6017559766769409\n",
      "epoch: 4, step: 40, loss: 0.5656389594078064\n",
      "epoch: 4, step: 50, loss: 0.6689474582672119\n",
      "epoch: 4, step: 60, loss: 0.5550901293754578\n",
      "epoch: 4, step: 70, loss: 0.6104570031166077\n",
      "epoch: 4, step: 80, loss: 0.5755224823951721\n",
      "epoch: 4, step: 90, loss: 0.5509076714515686\n",
      "epoch: 4, step: 100, loss: 0.5758857727050781\n",
      "epoch: 4, step: 110, loss: 0.5331305861473083\n",
      "epoch: 4, step: 120, loss: 0.6555162668228149\n",
      "epoch: 4, step: 130, loss: 0.5442023277282715\n",
      "epoch: 4, step: 140, loss: 0.5360397696495056\n",
      "epoch: 4, step: 150, loss: 0.5628937482833862\n",
      "epoch: 4, step: 160, loss: 0.5997341275215149\n",
      "epoch: 4, step: 170, loss: 0.5944527387619019\n",
      "epoch: 4, step: 180, loss: 0.6253266334533691\n",
      "epoch: 4, step: 190, loss: 0.532372236251831\n",
      "epoch: 4, step: 200, loss: 0.5875579714775085\n",
      "epoch: 4, step: 210, loss: 0.5811569690704346\n",
      "epoch: 4, step: 220, loss: 0.613315224647522\n",
      "epoch: 4, step: 230, loss: 0.6256411075592041\n",
      "epoch: 4, step: 240, loss: 0.6098571419715881\n",
      "epoch: 4, step: 250, loss: 0.5766623020172119\n",
      "epoch: 4, step: 260, loss: 0.6314378380775452\n",
      "epoch: 4, step: 270, loss: 0.5387787818908691\n",
      "epoch: 4, step: 280, loss: 0.6248798370361328\n",
      "epoch: 4, step: 290, loss: 0.5623530745506287\n",
      "epoch: 4, step: 300, loss: 0.5742822289466858\n",
      "epoch: 4, step: 310, loss: 0.5532418489456177\n",
      "epoch: 4, step: 320, loss: 0.5643412470817566\n",
      "epoch: 4, step: 330, loss: 0.5832592248916626\n",
      "epoch: 4, step: 340, loss: 0.5952069163322449\n",
      "epoch: 4, step: 350, loss: 0.5545847415924072\n",
      "epoch: 4, step: 360, loss: 0.5692950487136841\n",
      "epoch: 4, step: 370, loss: 0.7199488282203674\n",
      "epoch: 4, step: 380, loss: 0.6115729212760925\n",
      "epoch: 4, step: 390, loss: 0.5746817588806152\n",
      "epoch: 4, step: 400, loss: 0.5915467739105225\n",
      "epoch: 4, step: 410, loss: 0.608494758605957\n",
      "epoch: 4, step: 420, loss: 0.6826939582824707\n",
      "epoch: 4, step: 430, loss: 0.6528634428977966\n",
      "epoch: 4, step: 440, loss: 0.5875955820083618\n",
      "epoch: 4, step: 450, loss: 0.6772010326385498\n",
      "epoch: 4, step: 460, loss: 0.6250513792037964\n",
      "epoch: 4, step: 470, loss: 0.5861749649047852\n",
      "epoch: 4, step: 480, loss: 0.6167953610420227\n",
      "epoch: 4, step: 490, loss: 0.6353451609611511\n",
      "epoch: 4, step: 500, loss: 0.5971510410308838\n",
      "epoch: 4, step: 510, loss: 0.6516385078430176\n",
      "epoch: 4, step: 520, loss: 0.6613156199455261\n",
      "epoch: 4, step: 530, loss: 0.6837295889854431\n",
      "epoch: 4, step: 540, loss: 0.6923747658729553\n",
      "epoch: 4, step: 550, loss: 0.6638036370277405\n",
      "epoch: 4, step: 560, loss: 0.7385342121124268\n",
      "epoch: 4, step: 570, loss: 0.732617974281311\n",
      "epoch: 4, step: 580, loss: 0.6170827150344849\n",
      "epoch: 4, step: 590, loss: 0.7278539538383484\n",
      "epoch: 4, step: 600, loss: 0.650479257106781\n",
      "epoch: 4, step: 610, loss: 0.6918641924858093\n",
      "epoch: 4, step: 620, loss: 0.6448375582695007\n",
      "epoch: 4, step: 630, loss: 0.7589389085769653\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "# check dataset\n",
    "\n",
    "for i, (image, caption) in enumerate(dataloader):\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# torch.Size([2, 3, 256, 256]) torch.Size([2, 93])\n",
    "\n",
    "\n",
    "# 定义模型参数\n",
    "emb_dim = 512  # 嵌入维度\n",
    "nhead = 8  # 多头注意力的头数\n",
    "nhid = 1024  # 前馈网络的维度\n",
    "nlayers = 3  # 编码器和解码器层的数量\n",
    "vocab_size = vocab_size_len  # 词汇表大小\n",
    "max_seq_length = 512  # 最大序列长度\n",
    "\n",
    "cnn = FeatureExtractorCNN()\n",
    "transformer = ImageCaptioningTransformer(\n",
    "    emb_dim, nhead, nhid, nlayers, vocab_size, max_seq_length\n",
    ")\n",
    "\n",
    "# check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "cnn = cnn.to(device)\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "# train\n",
    "epoch = 5\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=lr)\n",
    "\n",
    "vocab_size = 160\n",
    "pad_index = 0\n",
    "\n",
    "# 使用权重创建损失函数\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "for e in range(epoch):\n",
    "    for i, (image, caption) in enumerate(dataloader):\n",
    "        image = image.to(device)\n",
    "        caption = caption.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        features = cnn(image)\n",
    "        \n",
    "        features = features.transpose(0, 1)\n",
    "        caption = caption.transpose(0, 1)\n",
    "        \n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(features, caption[:-1, :])\n",
    "        \n",
    "\n",
    "        output = transformer(features, caption[:-1, :], src_mask, tgt_mask, None, tgt_padding_mask)\n",
    "        \n",
    "        loss = criterion(output.reshape(-1, 160), caption[1:, :].reshape(-1))\n",
    "        \n",
    "        #check each pos\n",
    "        temp = 0\n",
    "        pad_num = 0\n",
    "        pad_num_2 = 0\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print(\"epoch: {}, step: {}, loss: {}\".format(e, i, loss.item()))\n",
    "            \n",
    "            #print output\n",
    "            # print(\"output: \", end=\"\")\n",
    "            # for i in range(92):\n",
    "            #     print(idx_to_word[torch.argmax(output[i])], end=\" \")\n",
    "            # print('\\n')\n",
    "    torch.save(transformer.state_dict(), \"./model/transformer_{}.pth\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, image_features, word2idx, idx2word,  max_length=93):\n",
    "\n",
    "    outputs = [word2idx[\"<start>\"]]\n",
    "\n",
    "    for i in range(max_length - 1):\n",
    "        \n",
    "        tgt_mask = (torch.triu(torch.ones(i + 1, i + 1)) == 1).transpose(0, 1)\n",
    "        tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            out = model(image_features, torch.tensor(outputs).unsqueeze(1).to(device), tgt_mask=tgt_mask.to(device))\n",
    "        \n",
    "        next_word = out.argmax(dim=2)[-1].item()\n",
    "\n",
    "        # 检查是否达到结束标记\n",
    "        if next_word == word2idx['<end>']:\n",
    "            break  # 一旦生成<end>标记，立即停止生成\n",
    "\n",
    "        outputs.append(next_word)\n",
    "        \n",
    "        # print output\n",
    "        print(\"output: \", end=\"\")\n",
    "        for i in range(len(outputs)):\n",
    "            print(idx2word[outputs[i]], end=\" \")\n",
    "        print('\\n')\n",
    "\n",
    "    # 转换序列为文字\n",
    "    caption = [idx2word[idx] for idx in outputs]\n",
    "\n",
    "    return ' '.join(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256]) torch.Size([95])\n",
      "output: <start> The \n",
      "\n",
      "output: <start> The person \n",
      "\n",
      "output: <start> The person is \n",
      "\n",
      "output: <start> The person is wearing \n",
      "\n",
      "output: <start> The person is wearing a \n",
      "\n",
      "output: <start> The person is wearing a sleeveless \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is an \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is an accessory \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is an accessory on \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is an accessory on her \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is an accessory on her wrist. \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is an accessory on her wrist. There \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is an accessory on her wrist. There is \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is an accessory on her wrist. There is a \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is an accessory on her wrist. There is a ring \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is an accessory on her wrist. There is a ring on \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is an accessory on her wrist. There is a ring on her \n",
      "\n",
      "output: <start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is an accessory on her wrist. There is a ring on her finger. \n",
      "\n",
      "<start> The person is wearing a sleeveless tank shirt with solid color patterns. The tank shirt is with cotton fabric. It has a suspenders neckline. The pants the person wears is of three-point length. The pants are with denim fabric and solid color patterns. There is an accessory on her wrist. There is a ring on her finger.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "transformer.load_state_dict(torch.load(\"./model/transformer_0.pth\"))\n",
    "transformer.eval()\n",
    "\n",
    "# test\n",
    "test_data = dataset[7]\n",
    "print(test_data[0].shape, test_data[1].shape)\n",
    "\n",
    "test_image = test_data[0].unsqueeze(0).to(device)\n",
    "test_feature = cnn(test_image)\n",
    "\n",
    "test_feature = test_feature.transpose(0, 1)\n",
    "\n",
    "output = generate_caption(transformer, test_feature, word_to_idx, idx_to_word)\n",
    "\n",
    "# output = transformer(test_feature, test_feature, None, None, None, None)\n",
    "\n",
    "print(output)\n",
    "\n",
    "# idx2word\n",
    "idx2word = {idx: word for idx, word in enumerate(words, 1)}\n",
    "idx2word[0] = \"<pad>\"\n",
    "idx2word[len(idx2word)] = \"<end>\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
