{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13720/4114127383.py:1: DeprecationWarning: 'cgi' is deprecated and slated for removal in Python 3.13\n",
      "  from cgi import test\n",
      "/home/dnull/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from cgi import test\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_path = \"./data/deepfashion-multimodal/images/\"\n",
    "train_text = \"./data/deepfashion-multimodal/train_captions.json\"\n",
    "test_text = \"./data/deepfashion-multimodal/test_captions.json\"\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_paths, train_captions, test_captions, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.train_captions = train_captions\n",
    "        self.test_captions = test_captions\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_name = self.image_paths[idx]\n",
    "        file_name = idx_name.split(\"/\")[-1]\n",
    "        image = Image.open(idx_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.train_captions[file_name]\n",
    "    \n",
    "    def get_train_captions(self):\n",
    "        return self.train_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train_captions = json.load(open(train_text, 'r'))\n",
    "test_captions = json.load(open(test_text, 'r'))\n",
    "image_paths = []\n",
    "# add from train_captions\n",
    "for key in train_captions.keys():\n",
    "    image_paths.append(image_path + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256 * 4, 256 * 4)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WOMEN-Jackets_Coats-id_00005611-01_4_full.jpg [159, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 13, 14, 15, 16, 17, 1, 18, 3, 16, 14, 5, 19, 1, 8, 16, 20, 9, 15, 4, 10, 11, 12, 21, 22, 23, 24, 25, 26, 27, 28, 7, 8, 9, 29, 12, 21, 30, 16, 31, 32, 33, 34, 35, 36, 21, 30, 4, 37, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "WOMEN-Tees_Tanks-id_00005033-03_4_full.jpg [159, 38, 39, 40, 4, 41, 6, 42, 8, 9, 43, 12, 44, 4, 32, 45, 46, 1, 47, 24, 32, 5, 48, 1, 49, 50, 28, 20, 8, 9, 10, 11, 12, 1, 22, 24, 32, 51, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "WOMEN-Rompers_Jumpsuits-id_00000245-01_1_front.jpg [159, 38, 39, 52, 4, 41, 6, 7, 8, 9, 10, 11, 12, 44, 4, 32, 53, 46, 21, 54, 24, 32, 5, 55, 1, 56, 50, 28, 7, 8, 9, 10, 11, 12, 57, 16, 32, 33, 34, 35, 36, 1, 22, 24, 32, 58, 57, 16, 25, 59, 34, 35, 60, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 2\n",
      "WOMEN-Blouses_Shirts-id_00002333-02_7_additional.jpg [159, 38, 61, 4, 5, 6, 7, 8, 9, 10, 11, 12, 44, 4, 32, 45, 46, 21, 54, 24, 32, 5, 48, 1, 49, 50, 28, 7, 8, 9, 62, 12, 57, 16, 25, 59, 34, 35, 60, 57, 16, 32, 33, 34, 35, 36, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 3\n",
      "WOMEN-Blouses_Shirts-id_00002102-04_4_full.jpg [159, 1, 22, 16, 31, 32, 39, 39, 40, 28, 43, 12, 1, 39, 40, 16, 28, 7, 63, 1, 49, 64, 22, 24, 16, 14, 65, 19, 1, 49, 50, 28, 20, 8, 9, 66, 11, 12, 57, 16, 25, 59, 34, 35, 60, 57, 16, 32, 33, 34, 35, 36, 57, 16, 25, 59, 67, 68, 35, 69, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 4\n"
     ]
    }
   ],
   "source": [
    "image_dic = train_captions.keys()\n",
    "image_descriptions = train_captions.values()\n",
    "\n",
    "# build vocabulary\n",
    "vocab = Counter()\n",
    "for description in image_descriptions:\n",
    "    vocab.update(description.split())\n",
    "\n",
    "# remove words that occur less than threshold\n",
    "threshold = -1\n",
    "words = [word for word, count in vocab.items() if count >= threshold]\n",
    "\n",
    "# create a mapping from word to index and index to word\n",
    "idx_to_word = {idx: word for idx, word in enumerate(words, 1)}\n",
    "\n",
    "# add the start and end token to the vocabulary\n",
    "idx_to_word[0] = \"<pad>\"\n",
    "idx_to_word[len(idx_to_word)] = \"<end>\"\n",
    "idx_to_word[len(idx_to_word)] = \"<start>\"\n",
    "\n",
    "# add the end to the end pos\n",
    "for key, description in train_captions.items():\n",
    "    train_captions[key] = \"<start> \" + description + \" <end>\"\n",
    "\n",
    "# pad the descriptions with <pad>\n",
    "max_length = max(len(description.split()) for description in image_descriptions)\n",
    "for key, description in train_captions.items():\n",
    "    train_captions[key] = description + \" <pad>\" * (\n",
    "        max_length - len(description.split())\n",
    "    )\n",
    "for key, description in test_captions.items():\n",
    "    test_captions[key] = description + \" <pad>\" * (\n",
    "        max_length - len(description.split())\n",
    "    )\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in idx_to_word.items()}\n",
    "vocab_size_len = len(idx_to_word)\n",
    "\n",
    "\n",
    "# convert each word to its index\n",
    "train_captions = {\n",
    "    key: [word_to_idx[word] for word in value.split()]\n",
    "    for key, value in train_captions.items()\n",
    "}\n",
    "test_captions = {\n",
    "    key: [word_to_idx[word] for word in value.split()]\n",
    "    for key, value in test_captions.items()\n",
    "}\n",
    "\n",
    "\n",
    "for key, value, idx in zip(train_captions.keys(), train_captions.values(), range(5)):\n",
    "    print(key, value, idx)\n",
    "    if idx == 4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1024, 1024]) torch.Size([95])\n",
      "tensor([[[0.9922, 0.9922, 0.9922,  ..., 0.9686, 0.9686, 0.9686],\n",
      "         [0.9922, 0.9922, 0.9922,  ..., 0.9686, 0.9686, 0.9686],\n",
      "         [0.9843, 0.9922, 0.9922,  ..., 0.9686, 0.9686, 0.9686],\n",
      "         ...,\n",
      "         [0.9686, 0.9686, 0.9686,  ..., 0.9216, 0.9216, 0.9216],\n",
      "         [0.9686, 0.9686, 0.9686,  ..., 0.9216, 0.9216, 0.9216],\n",
      "         [0.9686, 0.9686, 0.9686,  ..., 0.9294, 0.9294, 0.9294]],\n",
      "\n",
      "        [[0.9922, 0.9922, 0.9922,  ..., 0.9686, 0.9686, 0.9686],\n",
      "         [0.9922, 0.9922, 0.9922,  ..., 0.9686, 0.9686, 0.9686],\n",
      "         [0.9843, 0.9922, 0.9922,  ..., 0.9686, 0.9686, 0.9686],\n",
      "         ...,\n",
      "         [0.9529, 0.9529, 0.9529,  ..., 0.9059, 0.9059, 0.9059],\n",
      "         [0.9529, 0.9529, 0.9529,  ..., 0.9059, 0.9059, 0.9059],\n",
      "         [0.9529, 0.9529, 0.9529,  ..., 0.9216, 0.9216, 0.9216]],\n",
      "\n",
      "        [[0.9922, 0.9922, 0.9922,  ..., 0.9686, 0.9686, 0.9686],\n",
      "         [0.9922, 0.9922, 0.9922,  ..., 0.9686, 0.9686, 0.9686],\n",
      "         [0.9843, 0.9922, 0.9922,  ..., 0.9686, 0.9686, 0.9686],\n",
      "         ...,\n",
      "         [0.9608, 0.9608, 0.9608,  ..., 0.9137, 0.9137, 0.9137],\n",
      "         [0.9608, 0.9608, 0.9608,  ..., 0.9137, 0.9137, 0.9137],\n",
      "         [0.9608, 0.9608, 0.9608,  ..., 0.9059, 0.9059, 0.9059]]]) tensor([159,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,   1,\n",
      "         13,  14,  15,  16,  17,   1,  18,   3,  16,  14,   5,  19,   1,   8,\n",
      "         16,  20,   9,  15,   4,  10,  11,  12,  21,  22,  23,  24,  25,  26,\n",
      "         27,  28,   7,   8,   9,  29,  12,  21,  30,  16,  31,  32,  33,  34,\n",
      "         35,  36,  21,  30,   4,  37, 158,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key, value in train_captions.items():\n",
    "    train_captions[key] = torch.tensor(value, dtype=torch.long)\n",
    "\n",
    "# make dataset\n",
    "dataset = MyDataset(image_paths, train_captions, test_captions, transform)\n",
    "\n",
    "for i in range(1):\n",
    "    print(dataset[i][0].shape, dataset[i][1].shape)\n",
    "    print(dataset[i][0], dataset[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn import Transformer\n",
    "\n",
    "\n",
    "# 1. 使用预训练的 CNN 提取特征\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        \n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "\n",
    "    def forward(self, images):\n",
    "        # resize to 3 times larger\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        images = images.unfold(2, 256, 256).unfold(3, 256, 256)\n",
    "        images = images.contiguous().view(-1, 3, 256, 256)\n",
    "\n",
    "        # 提取特征\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(batch_size, 16, -1)\n",
    "        return features\n",
    "\n",
    "\n",
    "# 2. 构建 Transformer 模型\n",
    "class ImageCaptioningTransformer(nn.Module):\n",
    "    def __init__(self, emb_dim, nhead, nhid, nlayers, vocab_size, max_seq_length):\n",
    "        super(ImageCaptioningTransformer, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim)\n",
    "        self.transformer = Transformer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=1024,\n",
    "            num_encoder_layers=1,\n",
    "            num_decoder_layers=1,\n",
    "            batch_first=False,\n",
    "            dropout=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.decoder = nn.Linear(emb_dim, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        \n",
    "        self.trg_mask = None\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None,\n",
    "                tgt_padding_mask=None, memory_mask=None):\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(tgt):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(len(tgt)).to(tgt.device)\n",
    "\n",
    "        trg_pad_mask = self.make_len_mask(tgt)\n",
    "        \n",
    "        # output = self.encode(src, src_mask=src_mask, src_padding_mask=src_padding_mask)\n",
    "        output = src\n",
    "        output = self.decode(tgt, output, tgt_mask=self.trg_mask, tgt_key_padding_mask=trg_pad_mask,\n",
    "                                memory_mask=memory_mask, memory_key_padding_mask=src_padding_mask)\n",
    "        return output\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def make_len_mask(self, inp):\n",
    "        return (inp == 0).transpose(0, 1)\n",
    "    \n",
    "    def encode(self, src, src_mask=None, src_padding_mask=None):\n",
    "        src = self.pos_encoder(src)  # 添加位置编码\n",
    "        memory = self.transformer.encoder(src, mask=src_mask, src_key_padding_mask=src_padding_mask)\n",
    "        return memory\n",
    "    \n",
    "    def decode(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(tgt):\n",
    "            device = tgt.device\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(len(tgt)).to(device)\n",
    "\n",
    "        tgt = self.embedding(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer.decoder(tgt, memory, tgt_mask=self.trg_mask,\n",
    "                                          tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                          memory_mask=memory_mask,\n",
    "                                          memory_key_padding_mask=memory_key_padding_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "# 辅助类：位置编码\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 创建位置编码\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device='cuda')) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device='cuda').type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == 0).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == 0).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, image_features, word2idx, idx2word,  max_length=93):\n",
    "\n",
    "    outputs = [word2idx[\"<start>\"]]\n",
    "\n",
    "    for i in range(max_length - 1):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            out = model(image_features, torch.tensor(outputs).unsqueeze(1).to(device))\n",
    "        \n",
    "        next_word = out.argmax(dim=2)[-1].item()\n",
    "\n",
    "        # 检查是否达到结束标记\n",
    "        if next_word == word2idx['<end>']:\n",
    "            break  # 一旦生成<end>标记，立即停止生成\n",
    "        \n",
    "        # rand select next word from the top 1\n",
    "        next_word = torch.topk(out, 5, dim=2)[1][-1].squeeze().tolist()\n",
    "        # print next_word map to words\n",
    "        #print([idx2word[idx] for idx in next_word])\n",
    "        next_word = next_word[0]\n",
    "\n",
    "        outputs.append(next_word)\n",
    "\n",
    "    # 转换序列为文字\n",
    "    caption = [idx2word[idx] for idx in outputs]\n",
    "\n",
    "    return ' '.join(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnull/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dnull/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageCaptioningTransformer(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0, inplace=False)\n",
      "          (dropout2): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0, inplace=False)\n",
      "          (dropout2): Dropout(p=0, inplace=False)\n",
      "          (dropout3): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=160, bias=True)\n",
      "  (embedding): Embedding(160, 512)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnull/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FeatureExtractorCNN(\n",
       "  (resnet): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型参数\n",
    "emb_dim = 512  # 嵌入维度\n",
    "nhead = 8  # 多头注意力的头数\n",
    "nhid = 512  # 前馈网络的维度\n",
    "nlayers = 1  # 编码器和解码器层的数量\n",
    "vocab_size = vocab_size_len  # 词汇表大小\n",
    "max_seq_length = 512  # 最大序列长度\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "cnn = FeatureExtractorCNN().to(device)\n",
    "transformer = ImageCaptioningTransformer(\n",
    "    emb_dim, nhead, nhid, nlayers, vocab_size, max_seq_length\n",
    ").to(device)\n",
    "\n",
    "print(transformer)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "cnn = cnn.to(device)\n",
    "cnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ImageCaptioningTransformer:\n\tUnexpected key(s) in state_dict: \"encoder.weight\", \"encoder.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[1;32m      4\u001b[0m transformer \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model/transformer_thd3.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m      8\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ImageCaptioningTransformer:\n\tUnexpected key(s) in state_dict: \"encoder.weight\", \"encoder.bias\". "
     ]
    }
   ],
   "source": [
    "    \n",
    "# check device\n",
    "print(device)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "transformer.load_state_dict(torch.load(\"./model/transformer_thd3.pth\"))\n",
    "\n",
    "# train\n",
    "epoch = 3\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), 0.001)\n",
    "\n",
    "vocab_size = vocab_size_len\n",
    "\n",
    "print(vocab_size)\n",
    "pad_index = 0\n",
    "\n",
    "# 使用权重创建损失函数\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "for e in range(epoch):\n",
    "    for i, (image, caption) in enumerate(dataloader):\n",
    "        transformer.eval()\n",
    "        \n",
    "        \n",
    "        caption = caption.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        features = cnn(image.to(device))\n",
    "        \n",
    "        features = features.transpose(0, 1)\n",
    "        caption = caption.transpose(0, 1)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        output = None\n",
    "        \n",
    "        #check each pos\n",
    "        temp = 0\n",
    "        pad_num = 0\n",
    "        pad_num_2 = 0\n",
    "        \n",
    "        # split caption from length 2 to max_length\n",
    "        output = transformer(features, caption[:-1, :])\n",
    "        \n",
    "        loss = criterion(output.reshape(-1, 160), caption[1:, :].reshape(-1))\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print(\"epoch: {}, step: {}, loss: {}\".format(e, i, loss))\n",
    "\n",
    "                # output = output.transpose(0, 1)\n",
    "                # sample = output[0]\n",
    "                # sample = sample.argmax(dim=1)\n",
    "                # # print sentence\n",
    "                # sentence = \"\"\n",
    "                # for idx in sample:\n",
    "                #     if idx == 0:\n",
    "                #         pad_num += 1\n",
    "                #         continue\n",
    "                #     sentence += idx_to_word[idx.item()] + \" \"\n",
    "                # print(sentence)\n",
    "                # print(\"pad num: \", pad_num)\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print(caption.shape)\n",
    "        # ans = caption.transpose(0, 1)[0]\n",
    "        # # print sentence\n",
    "        # sentence = \"\"\n",
    "        # for idx in ans:\n",
    "        #     if idx == 0:\n",
    "        #         pad_num_2 += 1\n",
    "        #         continue\n",
    "        #     sentence += idx_to_word[idx.item()] + \" \"\n",
    "        # print(sentence)\n",
    "        \n",
    "        \n",
    "        transformer.eval()\n",
    "        \n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(generate_caption(transformer, features.transpose(0,1)[0].unsqueeze(1), word_to_idx, idx_to_word))\n",
    "            print(generate_caption(transformer, features.transpose(0,1)[1].unsqueeze(1), word_to_idx, idx_to_word))\n",
    "            # print(features.transpose(0,1)[0].unsqueeze(1))\n",
    "            # print(features.transpose(0,1)[1].unsqueeze(1))\n",
    "            # memory1 = transformer.encode(features.transpose(0,1)[0].unsqueeze(1))\n",
    "            # memory2 = transformer.encode(features.transpose(0,1)[1].unsqueeze(1))\n",
    "            # print(memory1)\n",
    "            # print(memory2)\n",
    "            # output1 = transformer.decode(torch.tensor([[0]]).to(device), memory1)\n",
    "            # output2 = transformer.decode(torch.tensor([[0]]).to(device), memory2)\n",
    "            # print(output1)\n",
    "            # print(output2)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "    torch.save(transformer.state_dict(), \"./model/transformer_thd{}.pth\".format(e+2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), \"./model/transformer_re{}.pth\".format(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1024, 1024]) torch.Size([95])\n",
      "<start> The shirt this female wears has medium sleeves and it is with cotton fabric and it has a round neckline. This female wears a long pants, with denim fabric and pure color patterns. There is a ring on her finger.\n",
      "<start> This woman wears a short-sleeve T-shirt with solid color patterns and a long trousers. The T-shirt is with cotton fabric. The neckline of the T-shirt is stand. The trousers are with denim fabric and solid color patterns. There is an accessory on her wrist. This lady wears a ring. There is a hat in her head. There is clothing on her waist. <end> \n"
     ]
    }
   ],
   "source": [
    "test_data = dataset[320]\n",
    "print(test_data[0].shape, test_data[1].shape)\n",
    "\n",
    "test_image = test_data[0].unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "test_feature = cnn(test_image)\n",
    "\n",
    "test_feature = test_feature.transpose(0, 1)\n",
    "\n",
    "transformer.load_state_dict(torch.load(\"./model/transformer_re4.pth\"))\n",
    "transformer.eval()\n",
    "\n",
    "output = generate_caption(transformer, test_feature, word_to_idx, idx_to_word)\n",
    "\n",
    "# output = transformer(test_feature, test_feature, None, None, None, None)\n",
    "\n",
    "print(output)\n",
    "\n",
    "caption = test_data[1].numpy()\n",
    "sentence = \"\"\n",
    "for idx in caption:\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    sentence += idx_to_word[idx.item()] + \" \"\n",
    "    \n",
    "print(sentence)\n",
    "\n",
    "# idx2word\n",
    "idx2word = {idx: word for idx, word in enumerate(words, 1)}\n",
    "idx2word[0] = \"<pad>\"\n",
    "idx2word[len(idx2word)] = \"<end>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnull/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transformer.eval()\n",
    "test_data = json.load(open(test_text, 'r'))\n",
    "\n",
    "test_image_paths = []\n",
    "# add from train_captions\n",
    "for key in test_data.keys():\n",
    "    test_image_paths.append(image_path + key)\n",
    "    \n",
    "res = {}\n",
    "    \n",
    "for i, path in enumerate(test_image_paths):\n",
    "    if i == 620:\n",
    "        break\n",
    "    \n",
    "    test_image = Image.open(path).convert(\"RGB\")\n",
    "    test_image = transform(test_image).unsqueeze(0).to(device)\n",
    "    test_feature = cnn(test_image)\n",
    "    \n",
    "    test_feature = test_feature.transpose(0, 1)\n",
    "    \n",
    "    output = generate_caption(transformer, test_feature, word_to_idx, idx_to_word)\n",
    "    \n",
    "    # remove start\n",
    "    output = output.split(\" \")\n",
    "    output = output[1:]\n",
    "    output = \" \".join(output)\n",
    "    \n",
    "    res[path.split(\"/\")[-1]] = output\n",
    "    \n",
    "json.dump(res, open(\"./result2.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up scorers...\n",
      "Calculating METEOR score...\n",
      "METEOR: 0.531\n",
      "Calculating ROUGE score...\n",
      "ROUGE: 0.490\n",
      "Calculating CIDER score...\n",
      "CIDER: 0.813\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "test_text = \"./data/deepfashion-multimodal/test_captions.json\"\n",
    "\n",
    "# read from result.json\n",
    "test_data = json.load(open(\"./result.json\", 'r'))\n",
    "real_data = json.load(open(test_text, 'r'))\n",
    "\n",
    "test_selected_data = {}\n",
    "real_selected_data = {}\n",
    "\n",
    "indice = 0\n",
    "for key, value in test_data.items():\n",
    "    real_selected_data[key] = real_data[key]\n",
    "    test_selected_data[key] = value\n",
    "    indice += 1\n",
    "\n",
    "eval = evaluate.DeepFashionEvalCap(real_selected_data, test_selected_data)\n",
    "eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
