{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务说明\n",
    "实现并评估基于网格表示的transfomer模型，用于对图像生成对应的文本描述\n",
    "\n",
    "## 实验数据\n",
    "基本的数据集使用课程提供的DeepFashion-MultiModal数据集，包含了对应的图像和文本描述\n",
    "\n",
    "## 实验环境\n",
    "- 操作系统： 22.04.1-Ubuntu x86_64 内核6.2.0-39-generic\n",
    "- GPU: NVIDIA GeForce RTX 3060 laptop GPU\n",
    "- CUDA: 12.2\n",
    "- python: conda-python 3.11.5\n",
    "- pytorch: 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cgi import test\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理阶段\n",
    "\n",
    "## 建立数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./data/deepfashion-multimodal/images/\"\n",
    "train_text = \"./data/deepfashion-multimodal/train_captions.json\"\n",
    "test_text = \"./data/deepfashion-multimodal/test_captions.json\"\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_paths, train_captions, test_captions, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.train_captions = train_captions\n",
    "        self.test_captions = test_captions\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_name = self.image_paths[idx]\n",
    "        file_name = idx_name.split(\"/\")[-1]\n",
    "        image = Image.open(idx_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.train_captions[file_name]\n",
    "    \n",
    "    def get_train_captions(self):\n",
    "        return self.train_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据集文本并建立图片的transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train_captions = json.load(open(train_text, 'r'))\n",
    "test_captions = json.load(open(test_text, 'r'))\n",
    "image_paths = []\n",
    "# add from train_captions\n",
    "for key in train_captions.keys():\n",
    "    image_paths.append(image_path + key)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256 * 4, 256 * 4)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对文本描述进行编码\n",
    "\n",
    "这一步将文本描述的每一个单词映射到一个整数, 并用一个整数序列来表示原本的文本描述\n",
    "\n",
    "具体的执行步骤如下:\n",
    "\n",
    "- 建立一个词典vocab, 用于将单词映射到整数\n",
    "- 将0映射为占位符<pad>\n",
    "- 将len(vocab)映射为未知单词<start>\n",
    "- 将len(vocab)+1映射为结束符<end>\n",
    "- 将原本的句子按照空格分割, 开头结尾加上<start>和<end>\n",
    "- 用<pad>将句子补齐到最大长度\n",
    "- 将句子中的每一个单词映射到整数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dic = train_captions.keys()\n",
    "image_descriptions = train_captions.values()\n",
    "\n",
    "# build vocabulary\n",
    "vocab = Counter()\n",
    "for description in image_descriptions:\n",
    "    vocab.update(description.split())\n",
    "\n",
    "# remove words that occur less than threshold\n",
    "threshold = -1\n",
    "words = [word for word, count in vocab.items() if count >= threshold]\n",
    "\n",
    "# create a mapping from word to index and index to word\n",
    "idx_to_word = {idx: word for idx, word in enumerate(words, 1)}\n",
    "\n",
    "# add the start and end token to the vocabulary\n",
    "idx_to_word[0] = \"<pad>\"\n",
    "idx_to_word[len(idx_to_word)] = \"<end>\"\n",
    "idx_to_word[len(idx_to_word)] = \"<start>\"\n",
    "\n",
    "# add the end to the end pos\n",
    "for key, description in train_captions.items():\n",
    "    train_captions[key] = \"<start> \" + description + \" <end>\"\n",
    "\n",
    "# pad the descriptions with <pad>\n",
    "max_length = max(len(description.split()) for description in image_descriptions)\n",
    "for key, description in train_captions.items():\n",
    "    train_captions[key] = description + \" <pad>\" * (\n",
    "        max_length - len(description.split())\n",
    "    )\n",
    "for key, description in test_captions.items():\n",
    "    test_captions[key] = description + \" <pad>\" * (\n",
    "        max_length - len(description.split())\n",
    "    )\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in idx_to_word.items()}\n",
    "vocab_size_len = len(idx_to_word)\n",
    "\n",
    "\n",
    "# convert each word to its index\n",
    "train_captions = {\n",
    "    key: [word_to_idx[word] for word in value.split()]\n",
    "    for key, value in train_captions.items()\n",
    "}\n",
    "test_captions = {\n",
    "    key: [word_to_idx[word] for word in value.split()]\n",
    "    for key, value in test_captions.items()\n",
    "}\n",
    "\n",
    "\n",
    "for key, value, idx in zip(train_captions.keys(), train_captions.values(), range(5)):\n",
    "    print(key, value, idx)\n",
    "    if idx == 4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in train_captions.items():\n",
    "    train_captions[key] = torch.tensor(value, dtype=torch.long)\n",
    "\n",
    "# make dataset\n",
    "dataset = MyDataset(image_paths, train_captions, test_captions, transform)\n",
    "\n",
    "for i in range(1):\n",
    "    print(dataset[i][0].shape, dataset[i][1].shape)\n",
    "    print(dataset[i][0], dataset[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "模型分为两个主要部分:\n",
    "- resnet18\n",
    "- transformer\n",
    "\n",
    "### resnet18\n",
    "resnet18能够刚好提取512维的特征, 这是transformer的嵌入维度的常用数值, 所以我们选择该模型作为图像特征提取器, 并使用预训练的参数, 在后续的训练过程中不再更新CNN部分的参数\n",
    "\n",
    "具体的, 我们通过transform将读取的图片resize到了(256*4, 256*4), 然后输入到FeatureExtractorCNN中, 然后将其切割为16块. 然后对于每一块我们单独使用resnet18提取其特征, 然后将这16个特征拼接起来, 得到一个(16, batch, 512)的输入序列.\n",
    "\n",
    "### transformer\n",
    "transformer部分的参数如下:\n",
    "```python\n",
    "self.transformer = Transformer(\n",
    "    d_model=emb_dim,\n",
    "    nhead=8,\n",
    "    dim_feedforward=1024,\n",
    "    num_encoder_layers=1,\n",
    "    num_decoder_layers=1,\n",
    "    batch_first=False,\n",
    "    dropout=0,\n",
    "    bias=True,\n",
    ")\n",
    "```\n",
    "但是在调试过程中我们发现, encoder部分无法有效学习到CNN部分提取的特征, 不论CNN给出的特征是多少, encoder都会输出一个相同的值, 在多次调参无果后, 我们决定删除encoder层, 直接将CNN提取的特征做为decoder的特征输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn import Transformer\n",
    "\n",
    "# 1. 使用预训练的 CNN 提取特征\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        \n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "\n",
    "    def forward(self, images):\n",
    "        # resize to 3 times larger\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        images = images.unfold(2, 256, 256).unfold(3, 256, 256)\n",
    "        images = images.contiguous().view(-1, 3, 256, 256)\n",
    "\n",
    "        # 提取特征\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(batch_size, 16, -1)\n",
    "        return features\n",
    "\n",
    "# 2. 构建 Transformer 模型\n",
    "class ImageCaptioningTransformer(nn.Module):\n",
    "    def __init__(self, emb_dim, nhead, nhid, nlayers, vocab_size, max_seq_length):\n",
    "        super(ImageCaptioningTransformer, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim)\n",
    "        self.transformer = Transformer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=1024,\n",
    "            num_encoder_layers=1,\n",
    "            num_decoder_layers=1,\n",
    "            batch_first=False,\n",
    "            dropout=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.decoder = nn.Linear(emb_dim, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        \n",
    "        self.trg_mask = None\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None,\n",
    "                tgt_padding_mask=None, memory_mask=None):\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(tgt):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(len(tgt)).to(tgt.device)\n",
    "\n",
    "        trg_pad_mask = self.make_len_mask(tgt)\n",
    "        \n",
    "        # output = self.encode(src, src_mask=src_mask, src_padding_mask=src_padding_mask)\n",
    "        output = src\n",
    "        output = self.decode(tgt, output, tgt_mask=self.trg_mask, tgt_key_padding_mask=trg_pad_mask,\n",
    "                                memory_mask=memory_mask, memory_key_padding_mask=src_padding_mask)\n",
    "        return output\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def make_len_mask(self, inp):\n",
    "        return (inp == 0).transpose(0, 1)\n",
    "    \n",
    "    def encode(self, src, src_mask=None, src_padding_mask=None):\n",
    "        src = self.pos_encoder(src)  # 添加位置编码\n",
    "        memory = self.transformer.encoder(src, mask=src_mask, src_key_padding_mask=src_padding_mask)\n",
    "        return memory\n",
    "    \n",
    "    def decode(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(tgt):\n",
    "            device = tgt.device\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(len(tgt)).to(device)\n",
    "\n",
    "        tgt = self.embedding(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer.decoder(tgt, memory, tgt_mask=self.trg_mask,\n",
    "                                          tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                          memory_mask=memory_mask,\n",
    "                                          memory_key_padding_mask=memory_key_padding_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "# 辅助类：位置编码\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 创建位置编码\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义生成描述的函数\n",
    "关于生成部分, 我们需要逐词生成描述, 并且在生成完毕后将其对照词典转化为单词, 生成从一个<start>开始, 当生成<end>时停止, 并且在生成过程中, 我们需要将生成的单词作为下一次的输入, 以此来实现逐词生成的目的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐词生成描述\n",
    "def generate_caption(model, image_features, word2idx, idx2word,  max_length=93):\n",
    "\n",
    "    outputs = [word2idx[\"<start>\"]]\n",
    "\n",
    "    for i in range(max_length - 1):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            out = model(image_features, torch.tensor(outputs).unsqueeze(1).to(device))\n",
    "        \n",
    "        next_word = out.argmax(dim=2)[-1].item()\n",
    "\n",
    "        # 检查是否达到结束标记\n",
    "        if next_word == word2idx['<end>']:\n",
    "            break  # 一旦生成<end>标记，立即停止生成\n",
    "        \n",
    "        # rand select next word from the top 1\n",
    "        next_word = torch.topk(out, 5, dim=2)[1][-1].squeeze().tolist()\n",
    "        # print next_word map to words\n",
    "        #print([idx2word[idx] for idx in next_word])\n",
    "        next_word = next_word[0]\n",
    "\n",
    "        outputs.append(next_word)\n",
    "\n",
    "    # 转换序列为文字\n",
    "    caption = [idx2word[idx] for idx in outputs]\n",
    "\n",
    "    return ' '.join(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "- 训练的数据batch_size为8, 这是由于计算资源的限制选择的最大值\n",
    "- epoch为5, 并且每个epoch训练完毕后都进行参数保存\n",
    "- 优化器使用Adam, 学习率为1e-3\n",
    "- 损失函数使用ce loss, 并且对于<pad>的部分不进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型参数\n",
    "emb_dim = 512  # 嵌入维度\n",
    "nhead = 8  # 多头注意力的头数\n",
    "nhid = 512  # 前馈网络的维度\n",
    "nlayers = 1  # 编码器和解码器层的数量\n",
    "vocab_size = vocab_size_len  # 词汇表大小\n",
    "max_seq_length = 512  # 最大序列长度\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 创建模型实例\n",
    "cnn = FeatureExtractorCNN().to(device)\n",
    "transformer = ImageCaptioningTransformer(\n",
    "    emb_dim, nhead, nhid, nlayers, vocab_size, max_seq_length\n",
    ").to(device)\n",
    "\n",
    "print(transformer)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "cnn = cnn.to(device)\n",
    "cnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transformer.to(device)\n",
    "transformer.load_state_dict(torch.load(\"./model/transformer_re4.pth\"))\n",
    "\n",
    "# train\n",
    "epoch = 5\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), 0.001)\n",
    "\n",
    "vocab_size = vocab_size_len\n",
    "\n",
    "# 使用权重创建损失函数\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "for e in range(epoch):\n",
    "    for i, (image, caption) in enumerate(dataloader):\n",
    "        transformer.eval()\n",
    "        \n",
    "        \n",
    "        caption = caption.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # 提取图像特征\n",
    "        features = cnn(image.to(device))\n",
    "        \n",
    "        features = features.transpose(0, 1)\n",
    "        caption = caption.transpose(0, 1)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        output = None\n",
    "        \n",
    "        #check each pos\n",
    "        temp = 0\n",
    "        pad_num = 0\n",
    "        pad_num_2 = 0\n",
    "        \n",
    "        # split caption from length 2 to max_length\n",
    "        output = transformer(features, caption[:-1, :])\n",
    "        \n",
    "        loss = criterion(output.reshape(-1, 160), caption[1:, :].reshape(-1))\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print(\"epoch: {}, step: {}, loss: {}\".format(e, i, loss))\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    torch.save(transformer.state_dict(), \"./model/transformer_thd{}.pth\".format(e+2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 效果展示\n",
    "这个代码块可以选择其中一张图片进行展示, 然后生成描述文本并输出, 然后输出参考文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_data = dataset[330]\n",
    "print(test_data[0].shape, test_data[1].shape)\n",
    "\n",
    "test_image = test_data[0].unsqueeze(0).to(device)\n",
    "test_feature = cnn(test_image)\n",
    "test_feature = test_feature.transpose(0, 1)\n",
    "\n",
    "transformer.load_state_dict(torch.load(\"./model/transformer_re4.pth\"))\n",
    "transformer.eval()\n",
    "\n",
    "# show the img\n",
    "plt.imshow(test_data[0].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "\n",
    "output = generate_caption(transformer, test_feature, word_to_idx, idx_to_word)\n",
    "\n",
    "# output = transformer(test_feature, test_feature, None, None, None, None)\n",
    "\n",
    "print(\"output: \", output)\n",
    "\n",
    "caption = test_data[1].numpy()\n",
    "sentence = \"\"\n",
    "for idx in caption:\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    sentence += idx_to_word[idx.item()] + \" \"\n",
    "    \n",
    "print(\"reference: \", sentence)\n",
    "\n",
    "# idx2word\n",
    "idx2word = {idx: word for idx, word in enumerate(words, 1)}\n",
    "idx2word[0] = \"<pad>\"\n",
    "idx2word[len(idx2word)] = \"<end>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算全部测试集\n",
    "对测试集的全部图片进行描述生成, 并保存为json文件, 用于后续的指标计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "test_data = json.load(open(test_text, 'r'))\n",
    "\n",
    "test_image_paths = []\n",
    "# add from train_captions\n",
    "for key in test_data.keys():\n",
    "    test_image_paths.append(image_path + key)\n",
    "    \n",
    "res = {}\n",
    "    \n",
    "for i, path in enumerate(test_image_paths):\n",
    "    if i == 620:\n",
    "        break\n",
    "    \n",
    "    test_image = Image.open(path).convert(\"RGB\")\n",
    "    test_image = transform(test_image).unsqueeze(0).to(device)\n",
    "    test_feature = cnn(test_image)\n",
    "    \n",
    "    test_feature = test_feature.transpose(0, 1)\n",
    "    \n",
    "    output = generate_caption(transformer, test_feature, word_to_idx, idx_to_word)\n",
    "    \n",
    "    # remove start\n",
    "    output = output.split(\" \")\n",
    "    output = output[1:]\n",
    "    output = \" \".join(output)\n",
    "    \n",
    "    res[path.split(\"/\")[-1]] = output\n",
    "    \n",
    "json.dump(res, open(\"./result2.json\", 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算指标\n",
    "使用自己实现的指标计算函数对三种不同指标进行计算评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "test_text = \"./data/deepfashion-multimodal/test_captions.json\"\n",
    "\n",
    "# read from result.json\n",
    "test_data = json.load(open(\"./result.json\", 'r'))\n",
    "real_data = json.load(open(test_text, 'r'))\n",
    "\n",
    "test_selected_data = {}\n",
    "real_selected_data = {}\n",
    "\n",
    "indice = 0\n",
    "for key, value in test_data.items():\n",
    "    real_selected_data[key] = real_data[key]\n",
    "    test_selected_data[key] = value\n",
    "    indice += 1\n",
    "\n",
    "eval = evaluate.DeepFashionEvalCap(real_selected_data, test_selected_data)\n",
    "eval.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
